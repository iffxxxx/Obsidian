next : [[]]
## Principal Component Analysis (PCA)
- ### Model-based method
	서로 비슷한 항목이나 사용자를 찾는 대신 등급 데이터에서 예측을 추출하기 위해 데이터 과학 및 기계 학습 기술을 적용하기 때문에 이를 모델 기반 방법이라는 레이블로 묶을 것입니다.  
	  
	머신 러닝은 예측을 하기 위해 모델을 훈련시키는 것이기 때문에, 우리는 추천을 하는 문제를 똑같이 다룰 것입니다.  
	  
	사용자 등급 데이터를 사용하여 모델을 교육하고 해당 모델을 사용하여 사용자가 새 항목의 등급을 예측할 수 있습니다.  
	  
	이를 통해 SurpriseLib이 구축하는 등급 예측 아키텍처의 공간에 정면으로 배치할 수 있습니다. 비록 항상 가장 효율적인 접근 방식은 아니라는 것을 보여주었지만, 머신 러닝 알고리즘을 리포팅하여 등급 예측에 매우 탁월한 추천 시스템을 구축할 수 있습니다.  
	  
	그들이 좋은 상위 N 추천을 받았는지는 우리가 알아봐야 할 것입니다.  
	
- ### Matrix factorization
	행렬 인수 분해 범주에 속하는 다양한 기법이 있습니다.  
	  
	그들은 액션 영화나 로맨틱 영화와 같은 사용자와 아이템의 더 넓은 기능을 스스로 찾을 수 있습니다.  
	  
	그들은 단지 데이터에서 떨어지는 속성들을 설명하는 행렬들로 설명됩니다.  
	  
	일반적인 개념은 사용자와 영화를 각 기능의 서로 다른 양의 조합으로 설명하는 것입니다.  
	
	예를 들어, 밥은 80%의 액션 팬과 20%의 코미디 팬으로 정의됩니다.  
	  
	그러면 80% 정도의 액션과 20% 정도의 코미디가 혼합된 영화와 그를 비교해 볼 수 있을 것입니다.  
	  
- 어떻게 작동하는지 자세히 알아보겠습니다.  
	![[Pasted image 20231114231329.png]]
	달성하고자 하는 것에서 거꾸로 작업하는 것이 일반적으로 좋은 접근 방식입니다. 그렇다면 사용자의 등급을 예측할 때 우리는 무엇을 하려고 합니까?  
	
	정말 희박한 행렬을 채우는 것이라고 생각하시면 됩니다.  
	  
	사용자를 나타내는 행과 항목을 나타내는 열이 있는 2D 행렬에 존재하는 것으로 모든 등급을 생각할 수 있습니다. 우리의 경우 영화 같은 경우에는 사용자를 나타내는 행이 있습니다.  
	  
	문제는 이 행렬의 대부분의 세포가 알려지지 않았다는 것입니다.  
	  
	우리의 과제는 알려지지 않은 세포들을 예측으로 채우는 것입니다.  
	  
	자, 만약 우리가 결국 행렬을 생각해 내려고 한다면, 우리가 볼 수 있는 행렬에 효과적인 기계 학습 기술들이 있을 수도 있습니다.  
	  
	그러한 기술 중 하나는 주성분 분석 또는 PCA라고 불립니다
	
- ### Principal component analysis
	주성분 분석은 일반적으로 차원 축소 문제로 설명됩니다.  
	
	즉, 사용자가 평가할 수 있는 모든 영화와 같이 여러 차원에 존재하는 데이터를 영화의 장르와 같이 정확하게 묘사할 수 있는 더 작은 차원 집합으로 만들고자 합니다.  
	
	![[Pasted image 20231115000647.png]]
	처음에는 영화에서 어떻게 작동하는지 상상하기가 좀 어렵지만, 붓꽃에 대한 데이터 세트가 있어 PCA가 작동하는 좋은 예가 됩니다.  
	
	붓꽃을 자세히 보면, 겉에는 커다란 꽃잎들이 한 다발씩 있고 속에는 작은 꽃잎들이 있는 것을 볼 수 있습니다.  
	  
	그 안쪽 꽃잎들은 아예 꽃잎이라고 부르지 않고, 꽃받침이라고 부릅니다.  
	
	![[Pasted image 20231115004739.png]]
	따라서 특정 붓꽃의 모양을 설명하는 한 가지 방법은 꽃잎의 길이와 너비, 그리고 꽃받침의 길이와 너비입니다.  따라서 붓꽃 데이터 세트에서 작업하는 데이터를 살펴보면 측정한 홍채마다 꽃잎과 꽃잎의 길이와 너비가 있습니다.  
	  
	이는 총 4차원의 데이터입니다.  
	  
	우리의 약한 인간의 뇌는 4D 데이터의 플롯을 상상할 수 없으므로, 우선 꽃잎의 길이와 너비에 대해 생각해봅시다.
	
	![[Pasted image 20231115005052.png]]
	우리 데이터 세트에 있는 모든 홍채에 대해 여기에 표시되어 있습니다.  
	
	- #### Eigenvectors
		그 검은 화살표들은 이 데이터의 고유 벡터 **(Eigenvectors)** 라고 불리는 것입니다.  
		
		기본적으로 데이터의 분산과 그에 [직교하는 벡터](412_Orthogonal_Functions#Orthogonal_Functions)를 가장 잘 설명할 수 있는 벡터입니다. 
		  
		함께 데이터에 더 적합한 새로운 벡터 공간 또는 기저를 정의할 수 있습니다.  
		  
		이러한 고유 벡터는 데이터의 주요 구성 요소입니다.  
		  
		그래서 주성분 분석이라고 합니다.  
		  
		우리가 찾고자 하는 것은 이러한 고유 벡터에 의해 제공되는 우리의 데이터를 설명하는 이러한 주요 구성요소입니다.  
	 
	이 주성분들이 왜 유용한지 생각해 보겠습니다.  
	  
	우선, 우리는 그 고유 벡터로부터의 데이터의 분산을 살펴볼 수 있습니다.  
	  
	고유 벡터로부터의 거리는 하나의 숫자로, 꽃잎 길이와 꽃잎 폭에 대해 시작한 2차원 데이터를 재구성하는 데 꽤 능숙합니다.  
	  
	따라서 이러한 주요 구성요소를 식별하면 처음보다 더 적은 차원을 사용하여 데이터를 표현할 수 있습니다.  
	  
	또한 이러한 고유 벡터는 데이터에 고유한 흥미로운 특징을 찾는 방법을 가지고 있습니다.  
	  
	이 경우 기본적으로 홍채의 전체적인 크기가 홍채의 어떤 종인지를 구분하는 데 중요하며, 너비와 길이의 비율은 다소 일정하다는 것을 알게 되었습니다.  
	  
	따라서 이 고유 벡터를 따라가는 거리는 기본적으로 꽃의 크기를 측정하는 것입니다.  
	  
	수학은 bigness가 무엇을 의미하는지 전혀 알지 못하지만, 그것을 설명하는 데 중요한 것으로 보이는 데이터에서 숨겨진 특징을 정의하는 벡터를 찾았고, 그것은 우리가 bigness라고 부르는 것과 우연히 일치합니다.  
	  
	따라서 PCA를 특징 추출 도구로 생각할 수도 있습니다.  
	  
	잠재적 특징을 발견하는 특징을 우리는 특징이라고 부릅니다.  
	![[Pasted image 20231115010021.png]]
	PCA의 최종 결과는 4차원 홍채 데이터 세트에 나와 있습니다.  
	  
	PCA를 사용하여 데이터를 가장 잘 나타내는 4D 공간 내의 두 가지 차원을 식별하고 이 두 가지 차원으로 그림을 표시했습니다.  
	  
	PCA는 자체적으로 처음에 사용한 만큼의 치수를 돌려주지만, 최소량의 정보가 포함된 치수는 폐기하도록 선택할 수 있습니다.  
	  
	따라서 PCA에서 데이터에 대해 가장 적게 알려준 2차원을 폐기하면 4차원에서 2차원으로 감소할 수 있습니다.  
	  
	우리는 이 X축과 Y축, 두 가지 차원이 무엇을 나타내는지 정말로 말할 수 없습니다.  
	  
	우리가 확실히 알고 있는 것은 PCA가 데이터에서 추출한 일종의 잠재적 요인 또는 특징을 나타낸다는 것입니다.  
	  
	그들은 어떤 의미를 갖지만, 우리는 데이터를 조사해야만 그들에게 일종의 인간 꼬리표를 붙이려고 시도할 수 있습니다.
## Singular Value Decomposition
- ### PCA on movie ratings
	![[Pasted image 20231115010517.png]]
	따라서 4차원 Iris 데이터 세트에서 PCA를 실행할 수 있는 것처럼, 모든 차원이 영화인 다차원 영화 등급 데이터 세트에서도 PCA를 실행할 수 있습니다.  
	
	사용자가 있는 이 등급 행렬을 R 행이라고 합니다.  
	
	Iris 데이터 세트와 마찬가지로 PCA는 데이터의 변화를 가장 잘 설명하는 훨씬 더 작은 차원으로 압축할 수 있습니다.  
	
	그리고 종종 그것이 발견하는 차원은 인간이 영화와 연관 짓기 위해 배운 특징들에 해당합니다. 예를 들어, 액션-y는 어떻게 영화인가요?  
	
	개인이 영화를 다르게 평가하게 만드는 것이 무엇이든 PCA는 그러한 잠재적 특징을 찾아내어 데이터에서 추출할 것입니다.  
	
	PCA는 그들이 무엇을 의미하는지는 모르지만 그럼에도 불구하고 그들을 발견합니다.  
	
	![[Pasted image 20231115010740.png]]
	따라서 PCA에 이 예에서 3차원으로 표현할 수 있도록 증류를 요청할 수 있으며, 이는 PCA가 파악한 3가지 잠재적 기능으로 등급을 낮출 수 있습니다.  
	  
	PCA는 그들을 뭐라고 불러야 할지 모르지만, 그들이 액션, 공상 과학, 고전 장르에 대한 각자의 관심을 측정하는 수단이 된다고 가정해 보겠습니다.  
	  
	예를 들어, 우리는 밥이 그의 관심사에 있어서 액션 30%, 공상 과학 50%, 고전 20%로 정의된다고 생각할 수 있습니다.  
	
	이제 이 새로운 행렬의 열을 살펴봅니다.  
	
	각 열은 해당 기능을 구성하는 사용자에 대한 설명입니다.  
	
	행동은 밥 30% + 테드 10% + 앤 30%로 설명할 수 있습니다.  
	
	행렬 U라고 하자, 행렬 열은 우리가 생성한 각 잠재적 특징에 대한 일반적인 사용자를 설명합니다.  
	
	![[Pasted image 20231115010815.png]]
	사용자 등급 데이터에서 PCA를 실행하여 일반적인 종류의 사용자의 프로필을 찾을 수 있는 것과 마찬가지로, 일반적인 종류의 영화의 프로필을 찾기 위해 여러 가지를 뒤집고 PCA를 실행할 수 있습니다.  
	  
	영화가 행이고 사용자가 열이 되도록 입력 데이터를 다시 정렬하면 이렇게 됩니다.  
	  
	우리는 이것을 원래 등급 행렬의 전치, 또는 줄여서 R-T라고 부릅니다.  
	
	![[Pasted image 20231115011425.png]]
	이제 PCA를 실행하면 잠재된 기능을 다시 확인하고 각 개별 영화를 어떤 조합으로 설명할 수 있습니다.
	
	각 열은 이제 잠재적인 특징을 보이는 전형적인 영화에 대한 설명입니다.  
	
- ### Singular Value Decomposition (SVD)
	![[Pasted image 20231115011448.png]]
	다시 말하지만, 이것들은 본질적인 의미는 없지만, 실제로는 영화 장르에 따라 떨어질 수도 있습니다.  
	  
	하지만 실제로는 더 복잡합니다.  
	  
	이 결과 행렬을 우리가  전형적인 영화 M을 묘사합니다.  
	  
	그렇다면 일반적인 사용자와 일반적인 영화를 설명하는 행렬이 시청률을 예측하는 데 어떤 도움이 될까요?  
	  
	자, 일반적인 영화 매트릭스와 일반적인 사용자 매트릭스의 전치가 우리가 처음으로 시작한 원래 등급 매트릭스의 요소임이 밝혀졌습니다.  
	  
	따라서 M이 있고 U가 있으면 R을 재구성할 수 있습니다.  
	  
	그리고 R에 등급이 일부 누락되어 있다면, M과 U가 있으면 R에 빈칸을 모두 채울 수 있습니다.  
	  
	이것이 우리가 행렬 인수분해라고 부르는 이유입니다.  
	  
	교육 데이터를 예측하려는 등급의 요인인 작은 행렬로 설명합니다.  
	  
	- $\sum\limits$ 행렬
		중간에는 우리가 필요로 하는 것에 대해 이야기하지 않았던 시그마 행렬도 있습니다.  
		  
		단순한 대각선 행렬일 뿐입니다. 결국 우리가 얻은 값을 적절한 크기로 확장하는 역할만 합니다.  
		  
		스케일링 행렬을 M 또는 U에 곱하면 됩니다. 그러나 R을 두 행렬의 곱으로 생각할 수 있습니다. 만약 그것이 당신의 머리를 쉽게 감쌀 수 있다면 말입니다.  
	  
	따라서 이들 요소를 함께 곱하여 R을 한 번에 재구성할 수 있으며, 사용자와 항목의 모든 조합에 대한 등급을 얻을 수 있습니다.  
	  
	그러나 이러한 요인이 있으면 사용자의 경우 U의 관련 행의 점 곱을, 항목의 경우 M-T의 관련 열을 선택하여 특정 사용자 및 항목에 대한 등급을 예측할 수도 있습니다.  
	  
	이것이 바로 행렬 곱셈의 원리입니다.  
	  
	마지막으로 우리는 모든 것을 묶을 것입니다.  
	  
	이 과정에서 우리는 SVD라는 내장형 추천기를 몇 번 사용했고, 매우 정확한 결과를 얻는 것으로 알려져 있습니다.  
	  
	이는 대회의 선두주자들 사이에서 넷플릭스 상 동안 널리 사용되었습니다.
	
	SVD는 특이치 분해를 의미합니다.  
	  
	특이치 분해가 어떤 역할을 하는지 아십니까?  
	  
	U, 시그마, M-T를 동시에 매우 효율적으로 계산하는 방식입니다.  
	  
	따라서 SVD는 사용자와 항목 모두에서 PCA를 실행하여 필요한 등급 행렬의 요소인 필요한 행렬을 반환하는 것입니다.  
	  
	SVD는 그 세 가지 요소를 한 번에 모두 얻을 수 있는 방법일 뿐입니다.  
	  
	- 문제점
		  ![[Pasted image 20231115012317.png]]
		 원래 행렬 R에 값의 대부분이 없을 때 U와 M-T를 어떻게 계산할 수 있을까요?  
		  
		PCA의 대부분이 누락된 행렬에서는 PCA를 실행할 수 없으며, 완전한 행렬이어야 합니다.  
		  
		누락된 값을 어떤 종류의 평균값과 같은 합리적인 기본값으로 채울 수 있습니다. 이것이 처음에 사람들이 한 일입니다.  
		  
		하지만, 더 좋은 방법이 있습니다.  
		
		$R = U\Sigma M^T$
		모든 등급은 행렬 U의 일부 행과 행렬 M-T의 일부 열의 점 곱으로 설명될 수 있습니다.  
		
		![[Pasted image 20231115012548.png]]
		예를 들어, The Empire Strikes Back에 대한 Bob의 등급을 예측하려면 U의 Bob 행과 M-T의 Empire Strikes Back 열의 점곱으로 계산할 수 있습니다.  
		  
		따라서 U와 M-T에서 주어진 행과 열에 대해 알려진 등급이 적어도 일부 있다고 가정합니다.  
		  
		우리는 이것을 최소화 문제로 취급할 수 있는데, 여기서 우리는 R에서 알려진 등급의 오류를 가장 잘 최소화하는 완전한 행과 열의 값을 찾으려고 노력합니다.  
		  
		확률적 경사 하강법, 또는 줄여서 SGD(Stochastic Gradiendt Descent)와 같이 그런 종류의 일을 할 수 있는 많은 기계 학습 기술이 있습니다.  
		  
		기본적으로 최소 오류 값에 도달할 때까지 주어진 학습 속도로 계속 반복합니다.  
		  
		그리고 다시 말하지만, SGD는 그것을 하는 하나의 방법일 뿐입니다.  
		
		예를 들어 Apache Spark는 최소 제곱 교대(alternating least squares, ALS)라는 다른 기술을 사용합니다.  
		  
		갑자기 행렬 M과 U의 값을 학습하고 SVD가 수행하는 것을 직접 계산하지 않는 것에 대해 이야기하고 있기 때문에 여기서 혼란스러울 수 있습니다.  
		  
		우리가 SVD 권장 사항을 수행한다고 말할 때, 그것은 실제 SVD가 아닙니다. 왜냐하면 누락된 데이터로는 실제 SVD를 수행할 수 없기 때문입니다.  
		  
		이것은 넷플릭스 상을 위해 발명된 SVD에서 영감을 받은 알고리즘이지만, 실제로 순수한 SVD는 아닙니다.  
		  
		넷플릭스 상의 수상자는 SVD++라고 불리는 특정 변종 SVD와 제한된 볼츠만 머신이라고 불리는 또 다른 기술의 조합이었지만, 그것은 또 다른 섹션의 이야기입니다.
		
		그러나 중요한 점은 다음과 같습니다. 사용자와 항목 집합에 대한 모든 등급을 행렬 R로 생각할 수 있으며, 행렬 R은 사용자의 일반적인 범주와 함께 곱할 수 있는 항목을 설명하는 더 작은 행렬로 분해될 수 있습니다.  
		  
		그 행렬들을 얻는 빠른 방법은 SVD, 단수 값 분해라고 불리는 기술입니다. 일단 그 행렬들을 가지고 나면, 각 행렬로부터 점 곱을 얻는 것만으로 어떤 사용자에 의한 어떤 항목의 등급을 예측할 수 있습니다.  
		  
		SGD, 확률적 경사 하강법, 최소 제곱 교대인 ALS와 같은 기법은 결측 데이터가 있을 때 요인 행렬의 최상의 값을 학습하는 데 사용할 수 있습니다.
	
- ### Improving on SVD
	![[Pasted image 20231115021822.png]]
	
- ### Exercise_SVDTuning.py
	Tune the hyperparameters for SVD with the MovieLens data set.
	SVDBakeoff 스크립트를 수정하여 SVD와 함께 사용할 수 있는 최상의 하이퍼 파라미터를 수정
	
```run-python
from MovieLens import MovieLens
from surprise import SVD
from surprise import NormalPredictor
from Evaluator import Evaluator
from surprise.model_selection import GridSearchCV

import random
import numpy as np

def LoadMovieLensData():
    ml = MovieLens()
    print("Loading movie ratings...")
    data = ml.loadMovieLensLatestSmall()
    print("\nComputing movie popularity ranks so we can measure novelty later...")
    rankings = ml.getPopularityRanks()
    return (ml, data, rankings)

np.random.seed(0)
random.seed(0)

# Load up common data set for the recommender algorithms
(ml, evaluationData, rankings) = LoadMovieLensData()

print("Searching for best parameters...")
param_grid = {'n_epochs': [20, 30], 'lr_all': [0.005, 0.010],
              'n_factors': [50, 100]}
gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)

gs.fit(evaluationData)

# best RMSE score
print("Best RMSE score attained: ", gs.best_score['rmse'])

# combination of parameters that gave the best RMSE score
print(gs.best_params['rmse'])

# Construct an Evaluator to, you know, evaluate them
evaluator = Evaluator(evaluationData, rankings)

params = gs.best_params['rmse']
SVDtuned = SVD(n_epochs = params['n_epochs'], lr_all = params['lr_all'], n_factors = params['n_factors'])
evaluator.AddAlgorithm(SVDtuned, "SVD - Tuned")

SVDUntuned = SVD()
evaluator.AddAlgorithm(SVDUntuned, "SVD - Untuned")

# Just make random recommendations
Random = NormalPredictor()
evaluator.AddAlgorithm(Random, "Random")

# Fight!
evaluator.Evaluate(False)

evaluator.SampleTopNRecs(ml)
```
#### SVD tuning results
저는 20개의 에포크와 .0005의 학습률과 50개의 요소에 대해 결정했습니다.
![[Pasted image 20231116033806.png|500]]
- 저처럼 이상적인 수치를 추정하는 것이 아니라 이상적인 수치를 얼마나 지속적으로 반복하느냐에 따라 더 좋은 결과를 얻을 수 있을 것입니다.  
	
	이렇게 조정된 결과를 통해 RMSE 점수가 정말 향상되었음을 알 수 있습니다. 
	
	그 차이는 작지만, 결과적으로 매우 다른 최고급 추천 결과를 가져왔다는 점이 흥미롭습니다. 
	
	이 두 가지 결과 모두 좋다고 생각하지만, 완전히 다릅니다.  
	
	예를 들어, 반지의 제왕은 SVD의 기본 매개 변수를 사용할 때 전혀 나타나지 않지만 3부작의 세 영화 모두 튜닝할 때 거품이 났습니다.
	
	85번 사용자가 이 세 영화를 모두 좋아할 것이라고 예측하는 알고리즘은 정확할 수 있지만, 이는 결과가 충분히 다양하지 않다는 것을 보여주는 좋은 예입니다.  
	
	종종, 높은 다양성은 그다지 좋지 않은 무작위 추천의 신호이지만, 너무 낮은 다양성 또한 나쁜 것이 될 수 있습니다.  
	
	이런 경우에는 시리즈의 첫 번째 영화만 추천하고 나머지 두 개의 슬롯은 다른 제목으로 자유롭게 하는 것이 좋을 것입니다.  
	
	때로는 같은 시리즈의 영화를 한 편 이상 추천하지 않는 간단한 규칙이 변화를 가져올 수 있습니다.
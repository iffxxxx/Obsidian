## Gaussian Splatting
- NERF의 상위호환
- [link](https://huggingface.co/papers/2311.13384)
## RNN
### 기존과 다른점?
Vanilla Neural Networks과 달리
입력이 하나라도 출력이 많음
중간모델의 출력이 다시한번 입력이 됨
One to one
One to many
Many to one
Many to Many

### RNN hidden state
입력을 2개를 받음
RNN은 매스탭마다 loss
출력마다 loss를 걸 수도 있다
### LSTM

## Attention and Transformers
### Sequence to Sequence with RNNs and Attention
#### softmax
출력을 확률을 나타내는 수치로 예전의 출력물과 곱해주게 됨
어떤 히든 스테이지에 강조를 주어야 하는가?

어떤 부분을 중점으로 보여주는가? 어디를 주의해서 봐야하는가?

$c_1$값이 출력값에 따라 다르게 늘림.

### Image Captioning with RNN and Attention

#### Self Attention layer
#### Masked self-attention layer
뒤에 올 정보들은 현재 추론에 사용하지 않겠습니다
미래에 쓸 정보드를 마스크 아웃 시켰다.

이러한 과정을 거쳐서
#### Multi-head self attention layer
Multiple self-attention heads in parallel

### Imahe Captioning with transformers

### Image Captioning using **Only** transformers
트랜스포머의 인코더로 이미지를 자른 이유 = 토큰 베이스

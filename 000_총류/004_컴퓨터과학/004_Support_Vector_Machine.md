## 비선형 SVM 분류
이러한 제한 사항에 대한 해결 방법은 선형 분류 문제에서 비선형 분류를 가능하게 하는 **서포트 벡터 머신(Support Vector Machine, SVM)** 입니다. SVM에는 다항식 커널 함수, 가우시안 방사 기저 함수(RBF) 커널 함수가 있습니다. 

### 커널 트릭
커널 함수를 사용하여 **데이터를 고차원 공간으로 매핑**하고, 원래 데이터 공간에서는 비선형한 패턴을 가진 문제를 고차원에서 선형 패턴으로 바꾸는 것을 커널 트릭이라고 합니다. 커널 트릭은 수학적 기교에 가까워, 실제로는 특성을 추가하지 않으면서 다항식 특성을 많이 추가한 것과 같은 결과를 얻을 수 있습니다. 일반적으로 다항식 특성을 추가하려면 각 원래 특성의 조합으로 새로운 특성을 생성해야 합니다. 예를 들어, 2차 다항식을 만들기 위해 $x_1$과 $x_2$ 두 개의 원래 특성이 있을 때, 이를 $x_1^2$, $x_2^2$, $x_1x_2$와 같은 새로운 특성으로 변환해야 합니다. 이렇게 하면 차원이 늘어나고 계산 비용이 높아질 수 있습니다. 하지만 가우시안 RBF을 사용하면 고차원 공간으로 매핑하고, 이 공간에서 데이터 간의 유사도 또는 거리를 계산하여 패턴을 찾습니다. 이를 통해 추가 특성을 직접 계산하지 않으면서도 고차원 공간에서의 효과를 얻을 수 있습니다. 이를 통해 비선형 패턴을 다룰 수 있고, 연산 비용을 낮출 수 있습니다.
[출처](https://mldlcvmjw.tistory.com/203)
![[Pasted image 20231022192047.png]]
위와 같이 선형으로 분리할 수 없는 경우에는 다항식(Polynomial) 커널을 사용하여 해결할 수 있으며, 이는 2차원의 점을 3차원으로 계산함으로써 가능하다. 계산은 다음과 같은 식에 따라 3차원으로 표시한다.
$$(x,y)\rightarrow(\sqrt{2xy},x^2,y^2)$$

![[Pasted image 20231022192311.png]]
이후 다항식 커널로 계산한 데이터 포인트들을 3차원에로 시각화하면 다음과 같은 모양이 나타난다.

[출처](https://mldlcvmjw.tistory.com/203)
![[Pasted image 20231022182641.png]]
SVM을 사용할 땐 커널 트릭이라는 수학적 기교를 적용할 수 있습니다. 커널 트릭은 실제로는 특성을 추가하지 않으면서 다항식 특성을 많이 추가한 것과 같은 결과를 얻을 수 있습니다. 이는 어떤 특성도 추가하지 않기 때문에 엄청난 수의 특성 조합이 생기지 않습니다.

![[Pasted image 20231022182833.png]]
왼쪽은 3차 다항식 커널을 사용한 SVM 분류기이고 오른쪽 그래프는 10차 다항식 커널을 사용한 또 다른 SVM 분류기 입니다. 모델이 과대적합이라면 다항식의 차수를 줄여야 하고, 반대로 과소적합이라면 차수를 늘려야 합니다. 

```
from sklearn.svm import SVC 
poly_kernel_svm_clf=Pipeline([ 
	('scaler',StandardScaler()), 
	('svm_clf',SVC(kernel='poly',degree=3,coef0=1,C=5)) ]) 
poly_kernel_svm_clf.fit(X,y)

```
degree는 다항식의 차수를, coef0는 모델이 높은 차수와 낮은 차수에 얼마나 영향을 받을지 조절합니다.

### 유사도 함수
방사 기저 함수 (RBF: Radical Bias Function)이란 RBF 커널 또는 가우시안 커널이라 부르기도 하며, 이 함수가 앞서 언급한 3차원을 넘는(feature가 3개가 넘는) 데이터에 대해 분류를 하기 위해 사용된다.

각 샘플이 특정 **랜드마크**와 얼마나 닮았는지 측정하는 **유사도 함수**로 계산한 특성을 추가하는 것이다

![[Pasted image 20231022184114.png]]
예를 들어서 앞에서 본 1차원 데이터셋에 두개의 랜드마크 $x_1=-2$와 $x_1=1$을 추가하자. 그리고 $r=0.3$인 가우시안 방사 기저 함수(RBF)를 유사도 함수로 정의하겠다.

**가우시안 방사 기저 함수**
$$\phi(x,l)=exp(-\frac{||x-l||^2}{2\sigma^2})$$
- $x$는 데이터 포인트
- $l$ (랜드마크)는 RBF 함수의 중심점
- $\sigma$ 는 함수의 폭(너비)을 제어하는 매개변수
$$\phi_\gamma(x,l)=exp(-\gamma||x-l||^2)$$
- $\gamma=\frac{1}{2\omega^2}$ 함수의 폭을 제어하는 매개변수

이 함수의 값은 0(랜드마크에서 아주 멀리 떨어진 경우)부터 1(랜드마트와 같은 위치일 경우)까지 변화하며 종 모양으로 나타납니다. 이제 새로운 특성을 만들 준비가 되었습니다. 예를 들어 $x_1=-1$ 샘플을 보면 첫 번째 랜드마크에서 1만큼 떨어져 있고 두 번째 랜드마크에서 2만큼 떨어져 있습니다. 그러므로 새로 만든 특성은 $x_2=exp(-0.3*1^2)\approx0.74$와 $x_3=exp(-0.3*2^2)\approx0.30$입니다.

새로운 특성 값은 각 데이터 포인트의 랜드마크와의 유사도를 나타내며, 데이터 포인트가 각 랜드마크에 얼마나 가까운지 반영합니다.

랜드마크를 선택하는 간단한 방법은 데이터셋에 있는 모든 샘플 위치에 랜드마크를 설정하는 것이다. 이렇게 하면 차원이 매우 커지고 따라서 변환된 훈련 세트가 선형적으로 구분될 확률이 높다. 단점은 훈련 세트에 있는 n개의 특성을 가진 m개의 샘플이 m개의 특성을 가진 m개의 샘플로 변환된다는 것이다. 훈련 세트가 매우 클 경우 동일한 크기의 아주 많은 특성이 만들어진다.

즉 다항 특성 방식과 마찬가지로 유사도 특성 방식도 머신러닝 알고리즘에 유용하게 사용될 수 있다. 추가 특성을 모두 계산하려면 연산 비용이 많이 드는데 특히 훈련 세트가 클 경우 더 그렇다. 여기에서 커널 트릭을 사용하면 유사도 특성을 많이 추가하는 것과 같은 비슷한 결과를 얻을 수 있다. 

![[Pasted image 20231022182747.png]]
```
rbf_kernel_svm_clf = Pipeline([
	('scaler',StandardScaler()), 
	('svm_clf',SVC(kernel='rbf',gamma=5,C=0.01)) 
	])
 rbf_kernel_svm_clf.fit(X,y)
```
gamma($\gamma$)를 증가시키면 종 모양 그래프가 좁아져서 각 샘플의 영향 범위가 작아진다. 결정 경계가 조금 더 불규칙해지고 각 샘플을 따라 구불구불하게 휘어진다. 반대로 작은 $\gamma$값은 넓은 종 모양 그래프를 만들며 샘플이 넓은 범위에 걸쳐 영향을 주므로 결정 경계가 더 부드러워진다. 하이퍼파라미터 $\gamma$가 규제 역할을 한다. 모델이 과대적합일 경우엔 감소시켜야 하고 과소적합일 경우에는 증가시켜야 한다.
## What is machine learning
톰 미첼의 유명한 정의에 따르면 기계학습은 다음과 같습니다.
> "만약 컴퓨터 프로그램이 어떤 작업 T의 일부로서 경험 E로부터 배우며, 성능 측정 P에 대한 측정을 통해 경험 E에서 성능이 향상된다면, 그것은 경험 E에 대해 학습한다고 말합니다."

이 책에서는 일반적인 종류의 기계 학습을 확률적 관점에서 다룰 것입니다.
- 접근을 취하는 이유는 다음과 같습니다:
	1. 불확실성 하에서의 의사 결정에 최적의 접근 방법
	2. 확률적 모델링은 대부분의 과학 및 공학 분야에서 사용되는 언어

## Supervised learning
- **정의:** 가장 흔한 기계 학습 형태는 지도 학습입니다. 이 문제에서 작업 T는 입력 x ∈ X에서 출력 y ∈ Y로의 매핑 f를 학습하는 것입니다.
	
	경험 E은 학습 세트로 알려진 N개의 입력-출력 쌍 $D = {(x_n, y_n)}_{n=1}^{N}$로 제공됩니다. (N은 샘플 크기라고 불립니다.) 성능 측정 P는 예측하려는 출력 유형에 따라 다르며, 아래에서 논의합니다.
	
- ### Classification
	 분류 문제에서 출력 공간은 C개의 정렬되지 않은 상호 배타적인 레이블로 구성된 클래스 집합입니다. 이를 Y = {1, 2, . . . , C}와 같이 나타냅니다. 입력이 주어졌을 때 클래스 레이블을 예측하는 문제는 패턴 인식이라고도 합니다. (클래스가 두 개뿐인 경우 $y ∈ {0, 1}$ 또는 $y ∈ {−1, +1}$과 같이 나타내며, 이는 이진 분류라고 불립니다.)
	 
	 - #### Image classification
		  이미지 분류에서 입력 공간 X는 이미지의 집합으로 매우 고차원 공간입니다. 예를 들어, 색 이미지의 경우 C = 3 채널 (예: RGB) 및 D1 × D2 픽셀이면 X = R^D이고, 여기서 D = C × D1 × D2입니다. (실제로는 각 픽셀 강도를 일반적으로 {0, 1, ..., 255} 범위에서 정수로 표현하지만, 표기의 간결성을 위해 실수 값 입력을 가정합니다.) 이미지에서 레이블로의 매핑 f : X → Y를 학습하는 것은 상당히 어려울 수 있습니다. 
		  ![[Pasted image 20240103172815.png]]
		  다행히도 몇몇 식물학자들은 이미 세플 길이, 세플 폭, 꽃잎 길이, 꽃잎 폭과 같이 단순하지만 **매우 유용한 숫자 특성 4가지를 식별**해냈습니다. 이를 통해 세 가지 종류의 아이리스 꽃을 구별할 수 있습니다. 본 섹션에서는 간단함을 위해 이러한 훨씬 낮은 차원의 입력 공간 X = R^4를 사용할 것입니다. Iris 데이터 세트는 이러한 4가지 특성으로 설명되는 150개의 아이리스 꽃 예제 모음으로, 각 유형당 50개씩 있습니다. 이는 크기가 작고 이해하기 쉬워 예시로 자주 사용됩니다.
		
		특성의 작은 데이터 세트를 가질 때, 이를 N × D 행렬에 저장하는 것이 일반적이며, 각 행은 예제를 나타내고 각 열은 특성을 나타냅니다. 이를 설계 행렬이라고 합니다. 
		
	- #### Exploratory data analysis
		  ML 문제에 대처하기 전에, 일반적으로 탐색적 데이터 분석을 수행하는 것이 좋습니다. 이는 어떤 명백한 패턴이 있는지 확인하고(어떤 방법을 선택할 지에 대한 힌트를 제공할 수 있음) 데이터에 명백한 문제가 있는지(예: 레이블 노이즈 또는 이상치) 확인하는 데 도움이 됩니다.
		  ![[Pasted image 20240103173040.png]]
		  또한, 데이터를 더 잘 이해하기 위해 결정 트리(Decision Tree)나 다른 모델을 사용하여 시각화할 수 있습니다. 위의 그림은 Iris 데이터에 깊이가 2인 결정 트리를 적용한 예시입니다. 여기서 잎 노드는 예측된 클래스에 따라 색상이 부여되며, 각 상자 내에는 루트에서 해당 노드로 통과하는 훈련 샘플 수가 표시됩니다. 이를 통해 각 노드에 속한 각 클래스 값의 수를 알 수 있으며, 이 벡터를 정규화하여 각 노드에 대한 클래스 레이블 분포를 얻을 수 있습니다. 이를 통해 우리는 다수의 클래스를 선택할 수 있습니다.
		  
	- #### Expirical risk minimization
		 지도 학습의 목표는 분류 모델을 자동으로 생성하여 어떠한 입력에 대해서도 레이블을 신뢰성 있게 예측하는 것입니다. 이 작업의 성능을 측정하는 일반적인 방법은 훈련 세트에서의 오분류 비율을 기반으로 하는데, 이는 다음과 같이 정의됩니다:
		
		$L(\theta)=\frac{1}{N}\sum_{n=1}{N}​I(y_n​\ne f(x_n​;\theta))$
		
		여기서 $I(e)$는 이진 표시 함수로, 조건 $e$가 참일 때만 1을 반환하고 그렇지 않으면 0을 반환합니다. 즉,
		
		$I(e)=\begin{cases}1,\;if\;e\;is\;true\\0,\;if\;e\;is\;false\end{cases}$
	 
	- #### Uncertainty
		  우리는 입력과 출력 간의 매핑에 대한 지식 부족으로 인해(이를 '에피스템적 불확실성' 또는 '모델 불확실성'이라고 함) 또는 매핑에서 내재된(줄일 수 없는) 확률적 변동성으로 인해 정확한 출력을 완벽하게 예측할 수 없을 것입니다.
		  
		- **확인 방법:** 우리는 다음과 같은 조건부 확률 분포를 사용하여 불확실성을 캡처할 수 있습니다:
			$p(y=c|x;\theta)=f_c​(x;\theta)$
			
		- **제약:**$f_c​(x;\theta)$는 클래스 레이블 c의 확률을 반환하므로 각 c에 대해 $0≤f_c​≤1$ 및 $\sum_{c=1}^{C}​f_c​=1$이 필요합니다.
			  
		- **제약 해결방안:** 모델이 정규화되지 않은 로그-확률을 반환하도록 요구하는 것이 일반적입니다. 그런 다음 이를 소프트맥스 함수를 사용하여 확률로 변환할 수 있는데, 이는 다음과 같이 정의됩니다:
			  $softmax(a)=\bigg(\frac{e^{a_1}}{\sum_{c=1}^{1}e^{a_{c_{0}}}},\cdots,\frac{e^{a_C}}{\sum_{c=1}^{1}e^{a_{c_{0}}}}\bigg)$
			  
			이는 $R^C$를 $[0, 1]^C$로 매핑하며, $0≤softmax(a)_{c}​≤1$ 및 $\sum_{c=1}^{C}​softmax(a)_{c}​=1$ 제약을 충족시킵니다.
	
	- #### Maximum likelihood estimation
		  
- ### Regrression
	- #### Linear reggression
	- #### Polynomial reggression
	- #### Deep neural networks
## Unsupervised learning
- ### Clustering
- ### Discovering latent "factors of variation"
- ### Self-supervised learning
- ### Evaluationg unsupervised learning
## Reinforcement learning
## Data
- ### Some common image datasets
- ### Some common text datasets
- ### Preprocessing discrete input data
- ### Preprocessing text data
- ### Handling missing data
## Discussion
- ### The relationship between ML and other fields
- ### Structure of the book
- ### Caveats


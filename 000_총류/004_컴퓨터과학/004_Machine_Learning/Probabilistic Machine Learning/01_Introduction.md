## What is machine learning
톰 미첼의 유명한 정의에 따르면 기계학습은 다음과 같습니다.
> "만약 컴퓨터 프로그램이 어떤 작업 T의 일부로서 경험 E로부터 배우며, 성능 측정 P에 대한 측정을 통해 경험 E에서 성능이 향상된다면, 그것은 경험 E에 대해 학습한다고 말합니다."

이 책에서는 일반적인 종류의 기계 학습을 확률적 관점에서 다룰 것입니다.
- 접근을 취하는 이유는 다음과 같습니다:
	1. 불확실성 하에서의 의사 결정에 최적의 접근 방법
	2. 확률적 모델링은 대부분의 과학 및 공학 분야에서 사용되는 언어

## Supervised learning
- **정의:** 가장 흔한 기계 학습 형태는 지도 학습입니다. 이 문제에서 작업 T는 입력 x ∈ X에서 출력 y ∈ Y로의 매핑 f를 학습하는 것입니다.
	
	경험 E은 학습 세트로 알려진 N개의 입력-출력 쌍 $D = {(x_n, y_n)}_{n=1}^{N}$로 제공됩니다. (N은 샘플 크기라고 불립니다.) 성능 측정 P는 예측하려는 출력 유형에 따라 다르며, 아래에서 논의합니다.
	
- ### Classification
	 분류 문제에서 출력 공간은 C개의 정렬되지 않은 상호 배타적인 레이블로 구성된 클래스 집합입니다. 이를 Y = {1, 2, . . . , C}와 같이 나타냅니다. 입력이 주어졌을 때 클래스 레이블을 예측하는 문제는 패턴 인식이라고도 합니다. (클래스가 두 개뿐인 경우 $y ∈ {0, 1}$ 또는 $y ∈ {−1, +1}$과 같이 나타내며, 이는 이진 분류라고 불립니다.)
	 
	 - #### Image classification
		  이미지 분류에서 입력 공간 X는 이미지의 집합으로 매우 고차원 공간입니다. 예를 들어, 색 이미지의 경우 C = 3 채널 (예: RGB) 및 D1 × D2 픽셀이면 X = R^D이고, 여기서 D = C × D1 × D2입니다. (실제로는 각 픽셀 강도를 일반적으로 {0, 1, ..., 255} 범위에서 정수로 표현하지만, 표기의 간결성을 위해 실수 값 입력을 가정합니다.) 이미지에서 레이블로의 매핑 f : X → Y를 학습하는 것은 상당히 어려울 수 있습니다. 
		  ![[Pasted image 20240103172815.png]]
		  다행히도 몇몇 식물학자들은 이미 세플 길이, 세플 폭, 꽃잎 길이, 꽃잎 폭과 같이 단순하지만 **매우 유용한 숫자 특성 4가지를 식별**해냈습니다. 이를 통해 세 가지 종류의 아이리스 꽃을 구별할 수 있습니다. 본 섹션에서는 간단함을 위해 이러한 훨씬 낮은 차원의 입력 공간 X = R^4를 사용할 것입니다. Iris 데이터 세트는 이러한 4가지 특성으로 설명되는 150개의 아이리스 꽃 예제 모음으로, 각 유형당 50개씩 있습니다. 이는 크기가 작고 이해하기 쉬워 예시로 자주 사용됩니다.
		
		특성의 작은 데이터 세트를 가질 때, 이를 N × D 행렬에 저장하는 것이 일반적이며, 각 행은 예제를 나타내고 각 열은 특성을 나타냅니다. 이를 설계 행렬이라고 합니다. 
		
	- #### Exploratory data analysis
		  ML 문제에 대처하기 전에, 일반적으로 탐색적 데이터 분석을 수행하는 것이 좋습니다. 이는 어떤 명백한 패턴이 있는지 확인하고(어떤 방법을 선택할 지에 대한 힌트를 제공할 수 있음) 데이터에 명백한 문제가 있는지(예: 레이블 노이즈 또는 이상치) 확인하는 데 도움이 됩니다.
		  ![[Pasted image 20240103173040.png]]
		  또한, 데이터를 더 잘 이해하기 위해 결정 트리(Decision Tree)나 다른 모델을 사용하여 시각화할 수 있습니다. 위의 그림은 Iris 데이터에 깊이가 2인 결정 트리를 적용한 예시입니다. 여기서 잎 노드는 예측된 클래스에 따라 색상이 부여되며, 각 상자 내에는 루트에서 해당 노드로 통과하는 훈련 샘플 수가 표시됩니다. 이를 통해 각 노드에 속한 각 클래스 값의 수를 알 수 있으며, 이 벡터를 정규화하여 각 노드에 대한 클래스 레이블 분포를 얻을 수 있습니다. 이를 통해 우리는 다수의 클래스를 선택할 수 있습니다.
		  
	- #### Expirical risk minimization
		 지도 학습의 목표는 분류 모델을 자동으로 생성하여 어떠한 입력에 대해서도 레이블을 신뢰성 있게 예측하는 것입니다. 이 작업의 성능을 측정하는 일반적인 방법은 훈련 세트에서의 오분류 비율을 기반으로 하는데, 이는 다음과 같이 정의됩니다:
		
		$L(\theta)=\frac{1}{N}\sum_{n=1}{N}​I(y_n​\ne f(x_n​;\theta))$
		
		여기서 $I(e)$는 이진 표시 함수로, 조건 $e$가 참일 때만 1을 반환하고 그렇지 않으면 0을 반환합니다. 즉,
		
		$I(e)=\begin{cases}1,\;if\;e\;is\;true\\0,\;if\;e\;is\;false\end{cases}$
	 
	- #### Uncertainty
		  우리는 입력과 출력 간의 매핑에 대한 지식 부족으로 인해(이를 '에피스템적 불확실성' 또는 '모델 불확실성'이라고 함) 또는 매핑에서 내재된(줄일 수 없는) 확률적 변동성으로 인해 정확한 출력을 완벽하게 예측할 수 없을 것입니다.
		  
		- **확인 방법:** 우리는 다음과 같은 조건부 확률 분포를 사용하여 불확실성을 캡처할 수 있습니다:
			$p(y=c|x;\theta)=f_c​(x;\theta)$
			
		- **제약:**$f_c​(x;\theta)$는 클래스 레이블 c의 확률을 반환하므로 각 c에 대해 $0≤f_c​≤1$ 및 $\sum_{c=1}^{C}​f_c​=1$이 필요합니다.
			  
		- **제약 해결방안:** 모델이 정규화되지 않은 로그-확률을 반환하도록 요구하는 것이 일반적입니다. 그런 다음 이를 소프트맥스 함수를 사용하여 확률로 변환할 수 있는데, 이는 다음과 같이 정의됩니다:
			  $softmax(a)=\bigg(\frac{e^{a_1}}{\sum_{c=1}^{1}e^{a_{c_{0}}}},\cdots,\frac{e^{a_C}}{\sum_{c=1}^{1}e^{a_{c_{0}}}}\bigg)$
			  
			이는 $R^C$를 $[0, 1]^C$로 매핑하며, $0≤softmax(a)_{c}​≤1$ 및 $\sum_{c=1}^{C}​softmax(a)_{c}​=1$ 제약을 충족시킵니다.
	
	- #### Maximum likelihood estimation
		  확률적 모델을 적합시킬 때, 일반적으로 손실 함수로 음의 로그 확률을 사용하는 것이 흔합니다:
		  
		$L(y,f(x;θ))=−logp(y∣f(x;θ))$
		
		이에 대한 이유는 직관적으로는 좋은 모델(낮은 손실)은 각 해당 입력 x에 대해 실제 출력 y에 높은 확률을 할당하는 모델입니다. 훈련 세트의 평균 음의 로그 확률은 다음과 같습니다:
		
		$NLL(θ)=−\frac{1}{N}​\sum_{n=1}{N}​log\,p(y_n​|f(x_n​;\theta))$
		
		이를 음의 로그 우도라고 합니다. 이를 최소화하면 최대 우도 추정치(MLE)를 계산할 수 있습니다:
		
		$\theta_{MLE}​=argmin_{\theta}\,​NLL(\theta)$
		
		이는 데이터에 모델을 맞추는 매우 일반적인 방법 중 하나입니다.
- ### Regrression
	- **정의:** 회귀는 분류와 매우 유사합니다. 그러나 출력이 실수값이기 때문에 다른 손실 함수를 사용해야 합니다. 회귀에서 가장 일반적인 선택은 제곱 손실 또는 L2 손실을 사용하는 것입니다.
		  
		- **L2 Loss:** $L_2(y,\hat{y})=(y-\hat{y})^2$
			  이는 큰 잔차 $y-\hat{y}​$를 작은 것보다 더 크게 패널티를 부여합니다. 제곱 손실을 사용할 때의 경험적 위험은 평균 제곱 오차 또는 MSE와 동일합니다.
			
		- **MSE:** $MSE(\theta)=\frac{1}{N}\sum_{n=1}^{N}{(y_n-f(x_n;\theta))^{2}}$
		  
	- **출력문제:** 회귀문제에서 출력 분포가 가우시안 또는 정규 분포라고 가정하는 것이 일반적입니다.
		  $N(y∣μ,σ^2)=\frac{1}{2πσ^2​}​exp(−\frac{1}{2σ^2}​(y−μ)^2)$
		
		여기서 $\mu$는 평균, $σ^2$는 분산이며, $\sqrt{2πσ^2}$​는 밀도가 1이 되도록 하는 정규화 상수입니다. 회귀의 맥락에서는 평균을 입력에 따라 정의함으로써 다음 조건부 확률 분포를 얻을 수 있습니다.
		
		$NLL(\theta)=\frac{1}{2\sigma^{2}}MSE(\theta)+const$
		이를 NLL식에 대입할 경우 NLL은 MSE에 비례함을 볼 수 있습니다. 따라서 최대 우도 추정치를 계산하면 제곱 오차를 최소화하게 됩니다.
	- #### Linear reggression
		회귀 모델의 예로 Figure 1.5a의 1차원 데이터를 고려해 봅시다. 이 데이터는 다음과 같은 간단한 선형 회귀 모델로 적합시킬 수 있습니다.
		
		$f(x;\theta)=b+wx$
		![[Pasted image 20240103183840.png]]
		여기서 w는 기울기, b는 offset이며, $θ=(w,b)$는 모델의 모든 매개변수입니다. $\theta$를 조절하여 Figure 1.5b의 수직 선으로 표시된 제곱 오차의 합을 최소화할 수 있습니다. 최소 제곱 회귀의 목표는 다음과 같은 최소 제곱 솔루션을 찾는 것입니다.
		
		$\hat{\theta}=argminθ​MSE(θ)$
		
		다중 입력 특성이 있는 경우에는 다음과 같이 작성할 수 있습니다.
		
		$f(x;\theta)=b+w_{1}​x_{1}​+⋯+w_{D}​x_{D}​=b+w^{T}x$(1.24)
		
		여기서 θ=(w,b)입니다. 이것은 다중 선형 회귀라고 불립니다
	- #### Polynomial reggression
		- **문제정의:** 
			![[Pasted image 20240103184240.png]]  
			선형 모델은 데이터에 대한 매우 좋지 않은 적합성을 보여줍니다. 우리는 D 차수의 다항 회귀 모델을 사용하여 적합성을 향상시킬 수 있습니다. 이 모델은 다음과 같은 형태를 가지며, 여기서 $\phi(x)$는 입력에서 파생된 특성 벡터이며 다음과 같은 형태를 가집니다.
			
			$f(x;w)=w^{T}\phi(x)$
			
			$\phi(x)=[1,x,x2,\cdots,xD]$
			
			위의 그림의 예시를 통해 D = 2에서 더 나은 적합성이 나타나는 것을 확인할 수 있습니다. D를 증가시킬 수 있으며, 따라서 모델의 매개변수 수를 증가시킬 수 있습니다. 여기서 D = N - 1이면 데이터 포인트 당 하나의 매개변수가 있으므로 데이터를 완벽하게 보간할 수 있습니다.
	- #### Deep neural networks
		- **정의:** 다차원 다항 회귀 모델을 넘어서서 딥 뉴럴 네트워크(DNN)에 대한 소개가 이루어집니다. 다항식 확장을 통해 특성을 수동으로 변환했습니다. 이제 입력 특성에 대한 비선형 특징 추출을 자동으로 수행할 수 있는 강력한 모델을 만들어보겠습니다.
			
			이를 위해 입력 특성 추출기인 $\phi(x;w,V)$가 자체 세트의 매개변수 $V$를 가지도록 하고, 전체 모델은 다음과 같은 형식을 가지게 됩니다.
			
			$f(x;w,V)=w^T\phi(x;V)$
			
			이러한 방식으로 딥 뉴럴 네트워크는 여러 계층으로 구성된 모델로 표현됩니다. 각 계층은 간단한 함수들의 중첩으로 특징 추출기를 재귀적으로 분해합니다. 최종 계층은 선형 함수로 구성되며, 이렇게 구축된 DNN은 이미지에 대한 컨볼루션 신경망(CNN)이나 순차적인 데이터에 대한 순환 신경망(RNN)과 같은 변형이 포함될 수 있습니다.
	- #### Overfitting and gen
## Unsupervised learning
- ### Clustering
- ### Discovering latent "factors of variation"
- ### Self-supervised learning
- ### Evaluationg unsupervised learning
## Reinforcement learning
## Data
- ### Some common image datasets
- ### Some common text datasets
- ### Preprocessing discrete input data
- ### Preprocessing text data
- ### Handling missing data
## Discussion
- ### The relationship between ML and other fields
- ### Structure of the book
- ### Caveats


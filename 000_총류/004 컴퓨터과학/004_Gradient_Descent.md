## Gradient Descent
[출처](https://hi-guten-tag.tistory.com/205)
이 과정에서 사용되는 것이 **경사 하강법(Gradient Descent)** 입니다. 경사하강법은 현재 위치의 기울기가 음수라면 파라미터를 증가시켜 최솟값을 찾을 수 있습니다. 반대로 기울기가 양수라면 파라미터를 감소시키면 최솟값을 찾을 수 있습니다. 즉 미분과 다르게 **반복적으로 파라미터를 업데이트 하면서 최적화 과정을 진행**하므로 모델을 업데이트하는데 필요한 그래디언트(기울기) 정보를 효과적으로 얻을 수 있습니다. 경사하강법은 통상적으로 학습률과 기울기 정보를 혼합하여 내가 나아갈 방향과 거리를 결정합니다. 따라서 적절한 학습률(Learning Rate)를 조절하는 Scheduling 기법이 필요합니다. 또한 지역 최솟값에 수렴할 수 있는 가능성이 존재합니다. 기울기의 지수 가중 이동 평균을 사용하여 가중치를 업데이트하는 Adam 옵티마이저를 사용하여 학습률을 자동으로 조정하고, 전역 최소값을 찾을 수 있습니다.